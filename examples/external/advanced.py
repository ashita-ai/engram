#!/usr/bin/env python3
"""Advanced features demo - RIF, multi-hop, negation filtering.

Demonstrates Engram's advanced recall features:
- Retrieval-Induced Forgetting (RIF) - suppress competing memories
- Multi-hop reasoning via follow_links
- Negation filtering - exclude contradicted information
- Summarization status filtering
- All 6 memory types

Prerequisites:
    - Qdrant running: docker run -p 6333:6333 qdrant/qdrant
    - API key in .env: ENGRAM_OPENAI_API_KEY=sk-...
"""

import asyncio
import logging

from engram.service import EngramService
from engram.storage import EngramStorage
from engram.workflows import init_workflows, shutdown_workflows
from engram.workflows.consolidation import run_consolidation

# Suppress INFO logs from DBOS to keep output clean
logging.getLogger("dbos").setLevel(logging.WARNING)


async def cleanup_demo_data(storage: EngramStorage, user_id: str) -> None:
    """Delete all data for the demo user to ensure clean slate."""
    from qdrant_client import models

    collections = ["episodic", "semantic", "factual", "negation", "procedural"]
    for memory_type in collections:
        collection = f"engram_{memory_type}"
        try:
            await storage.client.delete(
                collection_name=collection,
                points_selector=models.FilterSelector(
                    filter=models.Filter(
                        must=[
                            models.FieldCondition(
                                key="user_id",
                                match=models.MatchValue(value=user_id),
                            )
                        ]
                    )
                ),
            )
        except Exception:
            pass  # Collection might not exist


async def main() -> None:
    print("=" * 70)
    print("Engram Advanced Features Demo")
    print("=" * 70)

    # Initialize durable workflows (DBOS or Temporal based on config)
    try:
        init_workflows()
        print("  [Durable workflows initialized]")
    except Exception as e:
        print(f"  [Durable workflows not available: {e}]")

    async with EngramService.create() as engram:
        # Demo identifiers
        user_id = "advanced_demo"
        org_id = "demo_org"
        session_id = "advanced_session_001"

        # Clean up any existing data from previous runs
        await cleanup_demo_data(engram.storage, user_id)

        # Verify cleanup worked
        stats_after_cleanup = await engram.storage.get_memory_stats(user_id)
        if stats_after_cleanup.episodes > 0 or stats_after_cleanup.negation > 0:
            print(
                f"  [WARNING: Cleanup incomplete - episodes={stats_after_cleanup.episodes}, negation={stats_after_cleanup.negation}]"
            )
        else:
            print("  [Previous demo data cleaned up]")

        # Clean up any existing data from previous runs
        await cleanup_demo_data(engram.storage, user_id)

        # Verify cleanup worked
        stats_after_cleanup = await engram.storage.get_memory_stats(user_id)
        if stats_after_cleanup.episodes > 0 or stats_after_cleanup.negation > 0:
            print(
                f"  [WARNING: Cleanup incomplete - episodes={stats_after_cleanup.episodes}, negation={stats_after_cleanup.negation}]"
            )
        else:
            print("  [Previous demo data cleaned up]")

        # =====================================================================
        # Setup: Create memories in batches to demonstrate links
        # =====================================================================
        print("\n1. CREATING TEST DATA (in batches for linking)")
        print("-" * 70)

        # BATCH 1: Basic info about Alex
        batch1 = [
            ("user", "I'm Alex, a backend developer at CloudScale."),
            ("user", "My email is alex@cloudscale.io"),
            ("user", "I've been coding for 10 years, mostly in Go and Python."),
            ("user", "I prefer PostgreSQL for databases. It's my go-to choice."),
        ]

        print("  Batch 1: Basic profile...")
        for role, content in batch1:
            await engram.encode(
                content=content, role=role, user_id=user_id, org_id=org_id, session_id=session_id
            )

        # Consolidate batch 1 â†’ creates first semantic memory
        result1 = await run_consolidation(
            storage=engram.storage, embedder=engram.embedder, user_id=user_id, org_id=org_id
        )
        print(f"    â†’ {len(batch1)} episodes â†’ {result1.semantic_memories_created} semantic")

        # BATCH 2: Technical preferences (will link to batch 1)
        batch2 = [
            ("user", "For caching, I always use Redis alongside PostgreSQL."),
            ("user", "I use Neovim as my primary editor with a custom config."),
            ("user", "My favorite framework is FastAPI for Python APIs."),
            ("user", "For Go, I prefer using the standard library over frameworks."),
        ]

        print("  Batch 2: Technical preferences...")
        for role, content in batch2:
            await engram.encode(
                content=content, role=role, user_id=user_id, org_id=org_id, session_id=session_id
            )

        # Consolidate batch 2 â†’ should create links to batch 1's semantic
        result2 = await run_consolidation(
            storage=engram.storage, embedder=engram.embedder, user_id=user_id, org_id=org_id
        )
        print(f"    â†’ {len(batch2)} episodes â†’ {result2.semantic_memories_created} semantic")
        print(f"    â†’ {result2.links_created} links created to existing memories")

        # BATCH 3: Negations and corrections
        batch3 = [
            ("user", "I used to use MongoDB heavily at my previous job."),
            ("user", "I don't use MongoDB anymore - too many scaling issues."),
            ("user", "I'm not interested in blockchain or Web3."),
            ("user", "I never use Windows for development, only Linux."),
        ]

        print("  Batch 3: Corrections and negations...")
        negations_detected = 0
        for role, content in batch3:
            result = await engram.encode(
                content=content, role=role, user_id=user_id, org_id=org_id, session_id=session_id
            )
            negations_detected += len(result.negations)
        print(f"    â†’ {len(batch3)} episodes, {negations_detected} negations detected")

        # Consolidate batch 3
        result3 = await run_consolidation(
            storage=engram.storage, embedder=engram.embedder, user_id=user_id, org_id=org_id
        )
        print(f"    â†’ {result3.semantic_memories_created} semantic, {result3.links_created} links")

        total_episodes = len(batch1) + len(batch2) + len(batch3)
        print(f"\n  TOTAL: {total_episodes} episodes stored")

        # =====================================================================
        # 2. MEMORY TYPES
        # =====================================================================
        print("\n\n2. MEMORY TYPES")
        print("-" * 70)
        print("  Engram stores 5 persistent memory types:\n")

        # Get actual counts from storage
        stats = await engram.storage.get_memory_stats(user_id)
        print(
            f"  Counts: episodes={stats.episodes}, facts={stats.facts}, "
            f"semantic={stats.semantic}, negation={stats.negation}"
        )

        # Show examples of each type
        for memory_type, desc in [
            ("episodic", "raw conversations"),
            ("factual", "pattern-extracted"),
            ("semantic", "LLM-summarized"),
            ("negation", "what is NOT true"),
        ]:
            results = await engram.recall(
                query="Alex developer PostgreSQL",
                user_id=user_id,
                memory_types=[memory_type],
                limit=2,
            )
            if results:
                print(f"\n  [{memory_type.upper()}] ({desc})")
                for r in results[:2]:
                    conf = f" ({r.confidence:.0%})" if r.confidence else ""
                    print(f"    {r.content[:55]}...{conf}")

        # =====================================================================
        # 3. NEGATION FILTERING
        # =====================================================================
        print("\n\n3. NEGATION FILTERING")
        print("-" * 70)
        print("  Negations track what is NOT true and filter contradicted info.\n")

        # Show stored negations
        print("  Stored negations:")
        negation_results = await engram.recall(
            query="don't use never",
            user_id=user_id,
            memory_types=["negation"],
            limit=5,
        )
        for r in negation_results:
            print(f"    âœ— {r.content}")

        # Query for databases - should show PostgreSQL preference, not MongoDB
        print('\n  Query: "database" (episodic only)')

        # Without negation filtering - includes the MongoDB statement
        results_unfiltered = await engram.recall(
            query="database",
            user_id=user_id,
            memory_types=["episodic"],
            apply_negation_filter=False,
            limit=4,
        )

        # With negation filtering - MongoDB should be filtered
        results_filtered = await engram.recall(
            query="database",
            user_id=user_id,
            memory_types=["episodic"],
            apply_negation_filter=True,
            limit=4,
        )

        print(f"\n  WITHOUT negation filter ({len(results_unfiltered)} results):")
        for r in results_unfiltered[:4]:
            flag = " â† CONTRADICTED" if "mongo" in r.content.lower() else ""
            print(f"    {r.content[:55]}...{flag}")

        print(f"\n  WITH negation filter ({len(results_filtered)} results):")
        for r in results_filtered[:4]:
            print(f"    {r.content[:55]}...")

        removed = len(results_unfiltered) - len(results_filtered)
        if removed > 0:
            print(f"\n  âœ“ Removed {removed} contradicted result(s) â€” fewer but accurate")
        print(
            "\n  Behavior: Returns fewer results rather than backfilling with irrelevant content."
        )

        # =====================================================================
        # 4. MULTI-HOP REASONING
        # =====================================================================
        print("\n\n4. MULTI-HOP REASONING")
        print("-" * 70)
        print("  Follow links to discover connected memories.\n")

        # Check what links exist
        semantic_mems = await engram.storage.list_semantic_memories(user_id)
        total_links = sum(len(s.related_ids) for s in semantic_mems)
        print(f"  Semantic memories: {len(semantic_mems)}")
        print(f"  Total links between them: {total_links}")

        if total_links > 0:
            # Show the linked memories
            for sem in semantic_mems:
                if sem.related_ids:
                    print(f"\n  Memory: {sem.content[:50]}...")
                    print(f"    Links to: {sem.related_ids}")

            # Query with link following
            results_no_links = await engram.recall(
                query="PostgreSQL database",
                user_id=user_id,
                memory_types=["semantic"],
                follow_links=False,
                limit=5,
            )

            results_with_links = await engram.recall(
                query="PostgreSQL database",
                user_id=user_id,
                memory_types=["semantic"],
                follow_links=True,
                max_hops=2,
                limit=5,
            )

            print("\n  Query 'PostgreSQL database':")
            print(f"    Without follow_links: {len(results_no_links)} results")
            print(f"    With follow_links: {len(results_with_links)} results")

            # Show hop distances
            linked_results = [r for r in results_with_links if r.hop_distance > 0]
            if linked_results:
                print("\n  Discovered via link traversal:")
                for r in linked_results:
                    print(f"    [hop {r.hop_distance}] {r.content[:45]}...")
        else:
            print("\n  Note: No links created in this run.")
            print(
                "  Links are created when LLM identifies relationships between semantic memories."
            )
            print("  Try running consolidation multiple times to build more connections.")

        # =====================================================================
        # 5. RETRIEVAL-INDUCED FORGETTING (RIF)
        # =====================================================================
        print("\n\n5. RETRIEVAL-INDUCED FORGETTING (RIF)")
        print("-" * 70)
        print("  Based on Anderson et al. (1994): retrieving memories")
        print("  suppresses similar non-retrieved memories.\n")

        # Get all semantic memories (these have confidence to decay)
        rif_query = "Alex developer preferences"
        semantic_results = await engram.recall(
            query=rif_query,
            user_id=user_id,
            memory_types=["semantic"],
            limit=10,
            rif_enabled=False,
        )

        if len(semantic_results) >= 2:
            print(f'  Query: "{rif_query}"')
            print(f"  BEFORE RIF ({len(semantic_results)} semantic memories):")
            conf_before = {}
            scores_before = {}
            for r in semantic_results:
                conf_before[r.memory_id] = r.confidence or 0
                scores_before[r.memory_id] = r.score
                print(f"    [{r.confidence:.0%}] (score={r.score:.2f}) {r.content[:40]}...")

            # Use threshold below the scores to ensure suppression happens
            rif_threshold = min(scores_before.values()) - 0.05
            rif_decay = 0.1

            # Retrieve top 1 with RIF enabled - others should be suppressed
            print("\n  Retrieving top 1 with RIF enabled (limit=1)...")
            print(
                f"  Non-retrieved memories with score â‰¥ {rif_threshold:.2f} lose {rif_decay:.0%} confidence"
            )

            rif_results = await engram.recall(
                query=rif_query,
                user_id=user_id,
                memory_types=["semantic"],
                limit=1,
                rif_enabled=True,
                rif_threshold=rif_threshold,
                rif_decay=rif_decay,
                apply_negation_filter=False,  # Disable to isolate RIF behavior
            )

            retrieved_ids = {r.memory_id for r in rif_results}
            print(f"\n  RETRIEVED ({len(rif_results)}):")
            for r in rif_results:
                print(f"    âœ“ [{r.confidence:.0%}] {r.content[:45]}...")

            # Check if competitors were suppressed
            after_rif = await engram.recall(
                query=rif_query,
                user_id=user_id,
                memory_types=["semantic"],
                limit=10,
                rif_enabled=False,
            )

            print("\n  AFTER RIF (competitors):")
            for r in after_rif:
                if r.memory_id not in retrieved_ids:
                    old = conf_before.get(r.memory_id, 0)
                    new = r.confidence or 0
                    if new < old:
                        print(f"    âœ— [{old:.0%}â†’{new:.0%}] {r.content[:40]}... SUPPRESSED")
                    else:
                        print(f"    â€¢ [{new:.0%}] {r.content[:45]}...")
        else:
            print("  Not enough semantic memories for RIF demo.")
            print("  RIF requires multiple similar memories that compete.")

        print("\n  Note: Episodic memories are exempt (immutable ground truth)")

        # =====================================================================
        # 6. SUMMARIZATION STATUS FILTERING
        # =====================================================================
        print("\n\n6. SUMMARIZATION STATUS FILTERING")
        print("-" * 70)
        print("  Filter by consolidation status.\n")
        print("  Episodes can be 'unsummarized' (not yet included in semantic summaries)")
        print("  or 'summarized' (already compressed into semantic memory).\n")

        # Add new episodes AFTER consolidation to show unsummarized state
        print("  Adding 2 new episodes (not yet summarized)...")
        await engram.encode(
            content="I'm also learning Kubernetes for container orchestration.",
            role="user",
            user_id=user_id,
            org_id=org_id,
            session_id=session_id,
        )
        await engram.encode(
            content="My preferred cloud provider is AWS.",
            role="user",
            user_id=user_id,
            org_id=org_id,
            session_id=session_id,
        )

        # Query including episodic to show summarized vs unsummarized
        results_all = await engram.recall(
            query="cloud containers",
            user_id=user_id,
            memory_types=["episodic"],
            freshness="best_effort",
            limit=10,
        )

        summarized = [r for r in results_all if r.staleness.value == "fresh"]
        unsummarized = [r for r in results_all if r.staleness.value == "stale"]

        print(f"\n  Query: 'cloud containers' â†’ {len(results_all)} episodes")
        print(f"    Summarized: {len(summarized)}")
        print(f"    Unsummarized: {len(unsummarized)}")

        if unsummarized:
            print("\n  UNSUMMARIZED (new ground truth, pending consolidation):")
            for r in unsummarized[:2]:
                print(f"    ðŸ“ {r.content[:55]}...")

        # Show filtering
        results_summarized_only = await engram.recall(
            query="cloud containers",
            user_id=user_id,
            memory_types=["episodic"],
            freshness="fresh_only",
            limit=10,
        )

        print(f"\n  Filtering to summarized-only: {len(results_summarized_only)} results")
        if len(results_summarized_only) < len(results_all):
            print("  âœ“ Unsummarized episodes excluded")

    print("\n" + "=" * 70)
    print("Advanced features demo complete!")
    print("=" * 70)

    # Cleanup durable workflows
    shutdown_workflows()


if __name__ == "__main__":
    asyncio.run(main())
